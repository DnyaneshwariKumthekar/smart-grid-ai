# Quick Reference - Day 8-9 Implementation

## ðŸ“‹ What You Have Now

| Component | Status | Details |
|-----------|--------|---------|
| Data Loader | âœ… | 400+ lines - Generate & preprocess synthetic data |
| StackingEnsemble | âœ… | 600+ lines - LSTM + Transformer + XGBoost |
| Training Script | âœ… | 200+ lines - Complete end-to-end pipeline |
| Documentation | âœ… | 300+ lines - Architecture & usage guide |
| Project Structure | âœ… | data/, models/, notebooks/, results/, tests/ |

## ðŸš€ To Run Training (Choose One):

### Easiest - Double-Click
```
run_day8_9.bat
```

### Manual - Terminal
```powershell
.\venv\Scripts\Activate.ps1
pip install -r requirements.txt
python train_day8_9.py
```

### Custom - Python
```python
from data_loader import generate_synthetic_data, preprocess_data
from models.ensemble import StackingEnsemble

df = generate_synthetic_data(n_samples=100000)
X_train, X_test, y_train, y_test, scaler = preprocess_data(df)

ensemble = StackingEnsemble()
ensemble.fit(X_train, y_train, epochs=20)
metrics = ensemble.evaluate(X_test, y_test)
print(f"MAPE: {metrics['MAPE']:.2f}%")
```

## ðŸ“Š Success Criteria

âœ… **MAPE < 8.0%** (Primary target)
- LSTM alone: ~8.7%
- Transformer alone: ~7.6%
- Ensemble: **< 8%** (combination should be better)

## ðŸ“ˆ What to Expect

| Metric | Expected |
|--------|----------|
| MAPE | 6-8% |
| RMSE | 30-50 |
| RÂ² | 0.80-0.90 |
| Runtime | 10-15 min |
| Memory | 2-4 GB |

## ðŸ“ Output Files After Running

```
results/
â”œâ”€â”€ day8_9_metrics.csv
â”‚   â”œâ”€â”€ MAPE
â”‚   â”œâ”€â”€ RMSE
â”‚   â”œâ”€â”€ MAE
â”‚   â””â”€â”€ R2
â””â”€â”€ day8_9_predictions_sample.csv
    â”œâ”€â”€ actual
    â”œâ”€â”€ predicted
    â”œâ”€â”€ error
    â””â”€â”€ mape_sample
```

## ðŸ” Architecture

```
Input (batch, 288, 32)
  â†“
  â”œâ†’ LSTM â†’ Pred_1
  â””â†’ Transformer â†’ Pred_2
  â†“
  Meta-Features [Pred_1, Pred_2]
  â†“
  XGBoost
  â†“
  Final Output
```

## ðŸŽ¯ Key Features

### Data (32 Features)
- **Consumption (4)**: total, industrial, commercial, residential
- **Generation (5)**: solar, wind, hydro, thermal, nuclear
- **Weather (5)**: temperature, humidity, wind, clouds, precipitation
- **Time (8)**: hour (sin/cos), day (sin/cos), month (sin/cos)
- **System (5)**: frequency, voltage, active_power, reactive_power, power_factor
- **Derived (5)**: demand_gap, renewable%, peak, weekend, load

### Ensemble Strategy
- **Base Models**: LSTM captures sequences, Transformer uses attention
- **Meta-Learner**: XGBoost learns how to combine them
- **Validation**: K-fold CV prevents data leakage

## âš¡ If Training is Slow

```python
# In train_day8_9.py, reduce size:
df = generate_synthetic_data(n_samples=50000)   # Instead of 100000
ensemble.fit(X_train, y_train, epochs=10)       # Instead of 20
```

## âš¡ If MAPE is > 8%

```python
# Try one of these:
ensemble.fit(X_train, y_train, epochs=50)       # More training
ensemble = StackingEnsemble(lstm_hidden=128)    # Larger model
df = generate_synthetic_data(n_samples=200000)  # More data
```

## ðŸ“š Documentation Files

| File | Purpose |
|------|---------|
| `DAY8_9_GUIDE.md` | Detailed architecture & explanations |
| `IMPLEMENTATION_STARTED.md` | What's been done & next steps |
| `DATASET_SPECIFICATION.md` | Data structure & features |
| `CODE_TEMPLATES.md` | Code for Days 10-28 |

## ðŸ”— Key Functions

### Data Loader
```python
generate_synthetic_data(n_samples=100000)      # Create data
preprocess_data(df, test_size=0.2)            # Prepare for training
create_sequences(X, y, 288)                   # Create sliding windows
```

### StackingEnsemble
```python
ensemble = StackingEnsemble()                  # Initialize
ensemble.fit(X_train, y_train, epochs=20)     # Train
y_pred = ensemble.predict(X_test)             # Predict
metrics = ensemble.evaluate(X_test, y_test)   # Get metrics
```

## âœ… Checklist Before Running

- [ ] Python 3.8+ installed
- [ ] Virtual environment activated (`.\venv\Scripts\Activate.ps1`)
- [ ] Requirements installed (`pip install -r requirements.txt`)
- [ ] 8GB+ RAM available
- [ ] GPU optional but recommended

## ðŸŽ“ What You'll Learn

- **Deep Learning**: LSTM, Transformer, attention mechanisms
- **Ensemble Methods**: Meta-learning, K-fold CV, stacking
- **Preprocessing**: Time series handling, normalization, sequencing
- **Validation**: Proper train/test splits, preventing data leakage
- **Production Code**: Clean, documented, testable Python

## ðŸ”„ Next (Days 10-11)

After getting MAPE < 8%:
1. Create `models/moe.py` (MixtureOfExperts)
2. Implement 3 specialist networks (short/medium/long term)
3. Add gating mechanism for expert selection
4. Target: **MAPE < 5%**

## â“ Quick Troubleshooting

| Problem | Solution |
|---------|----------|
| `ModuleNotFoundError: torch` | `pip install -r requirements.txt` |
| `CUDA out of memory` | Edit train_day8_9.py: `device = torch.device('cpu')` |
| Very slow training | Reduce `n_samples=50000` or `epochs=10` |
| MAPE > 8% | Increase `epochs=50` or `n_samples=200000` |

---

**Ready?** â†’ Run `python train_day8_9.py` now! ðŸš€
